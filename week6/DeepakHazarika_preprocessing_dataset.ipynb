{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing sentiment analysis dataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEi9A7EnKV43"
      },
      "source": [
        "\n",
        "### Load the required libraries for translation , synonyms\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VlbLvKDYDpV",
        "outputId": "62ede389-6112-45a5-ed55-757169b7d376"
      },
      "source": [
        "!pip install googletrans==4.0.0-rc1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading https://files.pythonhosted.org/packages/fa/0d/a5fe8fb53dbf401f8a34cef9439c4c5b8f5037e20123b3e731397808d839/googletrans-4.0.0rc1.tar.gz\n",
            "Collecting httpx==0.13.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/b4/698b284c6aed4d7c2b4fe3ba5df1fcf6093612423797e76fbb24890dd22f/httpx-0.13.3-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n",
            "Collecting httpcore==0.9.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/d5/e4ff9318693ac6101a2095e580908b591838c6f33df8d3ee8dd953ba96a8/httpcore-0.9.1-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.7MB/s \n",
            "\u001b[?25hCollecting hstspreload\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/50/606213e12fb49c5eb667df0936223dcaf461f94e215ea60244b2b1e9b039/hstspreload-2020.12.22-py3-none-any.whl (994kB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 12.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2020.12.5)\n",
            "Collecting sniffio\n",
            "  Downloading https://files.pythonhosted.org/packages/52/b0/7b2e028b63d092804b6794595871f936aafa5e9322dcaaad50ebf67445b3/sniffio-1.2.0-py3-none-any.whl\n",
            "Collecting rfc3986<2,>=1.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c4/e5/63ca2c4edf4e00657584608bee1001302bbf8c5f569340b78304f2f446cb/rfc3986-1.5.0-py2.py3-none-any.whl\n",
            "Collecting h11<0.10,>=0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/fd/3dad730b0f95e78aeeb742f96fa7bbecbdd56a58e405d3da440d5bfb90c6/h11-0.9.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.7MB/s \n",
            "\u001b[?25hCollecting h2==3.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/de/da019bcc539eeab02f6d45836f23858ac467f584bfec7a526ef200242afe/h2-3.2.0-py2.py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.8MB/s \n",
            "\u001b[?25hCollecting hpack<4,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/cc/e53517f4a1e13f74776ca93271caef378dadec14d71c61c949d759d3db69/hpack-3.0.0-py2.py3-none-any.whl\n",
            "Collecting hyperframe<6,>=5.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/19/0c/bf88182bcb5dce3094e2f3e4fe20db28a9928cb7bd5b08024030e4b140db/hyperframe-5.2.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-cp37-none-any.whl size=17417 sha256=d643e76c7b4df6245be4d620325285d32a007c7201ca415bb9fc0b3868c7cb68\n",
            "  Stored in directory: /root/.cache/pip/wheels/09/32/56/fd8940f1b3c1d77c9f91b55597c52a4d4833b000a980bb0740\n",
            "Successfully built googletrans\n",
            "Installing collected packages: sniffio, h11, hpack, hyperframe, h2, httpcore, hstspreload, rfc3986, httpx, googletrans\n",
            "Successfully installed googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2020.12.22 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 rfc3986-1.5.0 sniffio-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mB7_bVeL-kka",
        "outputId": "5ab54410-1be0-4b74-eea5-6ed34121ce5c"
      },
      "source": [
        "!pip install spacy-wordnet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy-wordnet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/f2/4d8070df0f7a7a9eeed74eb7e9ce3cf41349eb5e06b1e088de9eeca630e2/spacy-wordnet-0.0.4.tar.gz (648kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 5.1MB/s \n",
            "\u001b[?25hCollecting nltk<3.4,>=3.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/09/3b1755d528ad9156ee7243d52aa5cd2b809ef053a0f31b53d92853dd653a/nltk-3.3.0.zip (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 11.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk<3.4,>=3.3->spacy-wordnet) (1.15.0)\n",
            "Building wheels for collected packages: spacy-wordnet, nltk\n",
            "  Building wheel for spacy-wordnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spacy-wordnet: filename=spacy_wordnet-0.0.4-py2.py3-none-any.whl size=650293 sha256=0f7ad1666c230637c42189323daf18c9013889ce3150e50682a52511d0b560aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/93/1d/c86db913cd146fc9ddb26d10f56579c5d58a3e00bc8f96a3a6\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-cp37-none-any.whl size=1394470 sha256=b30315783f35e9380cf2d75b74312995bd9f195a9753f89721e2e3a40a524697\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/ab/40/3bceea46922767e42986aef7606a600538ca80de6062dc266c\n",
            "Successfully built spacy-wordnet nltk\n",
            "Installing collected packages: nltk, spacy-wordnet\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.3 spacy-wordnet-0.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9czGCwOcIi6"
      },
      "source": [
        "### Import the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lq3V1fJ38kG"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import re\n",
        "import pandas as pd\n",
        "import random\n",
        "from spacy.lang.en import English\n",
        "from datetime import datetime\n",
        "import googletrans\n",
        "from googletrans import Translator\n",
        "import spacy\n",
        "\n",
        "from spacy_wordnet.wordnet_annotator import WordnetAnnotator "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sxj2MP9Jphht",
        "outputId": "f746e412-9c33-4ee5-cf17-8071bb746359"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ztAvH3w5-6Q"
      },
      "source": [
        "### Code to get the label cardinality\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDikLEko59_N"
      },
      "source": [
        "import math\n",
        "values = [0.34 , 0.6 , 0.7 , 0.4]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPRDagdzgLNg"
      },
      "source": [
        "### algorithm for generating the raw dataset\n",
        "\n",
        "*   parse the sentiment_labels.txt and create a dictionary of Phrase_id to label\n",
        "*   parse the dictionary.txt and create a dictionary of sentence to phrase_id\n",
        "*   parse the datasetSentences.txt and extract the string id and the comment , store this in a dictionary and also create a dataframe of comment and label\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1Kw3Uamfxvl"
      },
      "source": [
        "counter = 0\n",
        "deletecounter = 0\n",
        "insertcounter = 0\n",
        "translatecounter = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25M0b_l7OC32"
      },
      "source": [
        "### Create a class StanfordSentimentAnalysis to encapsulate methods and attributes \n",
        "\n",
        "#### Following API's are supported\n",
        "\n",
        "1. getLabelIndex(self , value)\n",
        "\n",
        ">> Input : a float value\n",
        "\n",
        ">> Output : an integer label\n",
        "\n",
        "| range of float value | integer label |\n",
        "| --- | --- |\n",
        "| 0 <= value <= 0.2 | **0** |\n",
        "| 0.2 < value <= 0.4 | **1** |\n",
        "| 0.4 < value <= 0.6 | **2** |\n",
        "| 0.6 < value <= 0.8 | **3** |\n",
        "| 0.8 < value <= 1.0 | **4** |\n",
        "\n",
        "2. getSentences(self)\n",
        "\n",
        ">> This method will load the datasetSentences file and extract\n",
        "\n",
        ">>> string id's\n",
        "\n",
        ">>> review comments\n",
        "\n",
        ">> store this in a dataframe\n",
        "\n",
        "3. getPhraseIds(self):\n",
        "\n",
        ">> This method will load the dictionary file and extract\n",
        "\n",
        ">>> phrases \n",
        "\n",
        ">>> phrase ids\n",
        "\n",
        ">> store this in a dictionary\n",
        "\n",
        "4.  getSentimentLabels(self)\n",
        "\n",
        ">> This method will load the sentiment_labels file and extract\n",
        "\n",
        ">>> phrases ids \n",
        "\n",
        ">>> labels which are float value between 0 and 1 , including 0 and 1\n",
        "\n",
        ">> store this in a dictionary\n",
        "\n",
        "5.  getSentenceLabelsDF(self):\n",
        "\n",
        ">> This method finds label for the review comment , the algorithm for the same is given below:\n",
        "\n",
        ">>> go through the list of review comments\n",
        "\n",
        ">>> check if the sentence exists as a phrase in the dictionary\n",
        "\n",
        ">>>> if yes then get the phrase id and find the label for the given phrase id in labels dictionary\n",
        "\n",
        ">>> store the review comment and the label in a data frame object\n",
        "\n",
        "6.  getSentenceandLabels(self)\n",
        "\n",
        ">> This is a wrapper function which call\n",
        "\n",
        ">>> getSentimentLabels()\n",
        "\n",
        ">>> getPhraseIds()\n",
        "    \n",
        ">>> getSentences()\n",
        "\n",
        ">>> getSentenceLabelsDF()\n",
        "\n",
        ">> The result is stored in a sentence label data frame\n",
        "\n",
        "7.  getSpacyTokens(self , sentence):\n",
        "\n",
        ">> This is a helper method to get spacy tokens of a sentence, a rule has been included to ignore apostrophe as a separate token.\n",
        "\n",
        ">>> Input : Sentence\n",
        "\n",
        ">>> Output - tokens\n",
        "\n",
        "\n",
        "8.  random_deletion(self , sentence, p=0.3)\n",
        "\n",
        ">> This method randomly deletes words from a sentence based on a probability.If the probability is less then a threshold , then drop the word.\n",
        "\n",
        ">>> Input : Sentence\n",
        "\n",
        ">>> Output - modified sentence\n",
        "\n",
        "9.  random_swap(self , sentence, n=3):\n",
        "\n",
        ">> This method randomly swaps n number of words in a sentence.\n",
        "\n",
        ">>> Input : Sentence\n",
        "\n",
        ">>> Output - modified sentence\n",
        "\n",
        "10.  getReTranslatedSentence(self , sentence):\n",
        "\n",
        ">> This method uses google translation package to translate the sentence to a random destination language.WHich is then retranslated to english.\n",
        "\n",
        ">>> Input : Sentence\n",
        "\n",
        ">>> Output - modified sentence\n",
        "\n",
        "11.  removeStopWord(self, sentence):\n",
        "\n",
        ">> This method uses spacy token attributes to check if it is a stop word\n",
        "\n",
        ">>> If yes then drop the word\n",
        "\n",
        ">>> Input : Sentence\n",
        "\n",
        ">>> Output - modified sentence\n",
        "\n",
        "12.  insertSynonymNTimes(self , input , numTimes):\n",
        "\n",
        ">> This method is used to insert synonym for a word n number of times , this is a way to emphasise some words in the sentence.Wordnet library is used to get synonym for a word.The list of synonym is narrowed down based on domain.SInce these sentences are about movie , so the domain is set to celluloid etc.More about domain information can be found at domains - https://wndomains.fbk.eu/hierarchy.html\n",
        "\n",
        ">>> Input : Sentence\n",
        "\n",
        ">>> Output - modified sentence\n",
        "\n",
        "13.  random_insertion(self , sentence, howmanytimes = 3): \n",
        "\n",
        ">> This is a wrapper method which call the removestop word and insertSynonymNTimes.\n",
        "\n",
        ">>> Input : Sentence\n",
        "\n",
        ">>> Output - modified sentence\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33qaKEPigK2I"
      },
      "source": [
        "class StanfordSentimentAnalysis:\n",
        "  def __init__(self):\n",
        "\n",
        "    self.sentencesDF = pd.DataFrame()\n",
        "    self.phraseIdDict = {}\n",
        "    self.labelsDict = {}\n",
        "    self.sentencesLabelDF = pd.DataFrame()\n",
        "\n",
        "  def getLabelIndex(self , value):\n",
        "\n",
        "    num_of_bins= 5\n",
        "    bin_width = 0.2\n",
        "    bins = [0] * num_of_bins\n",
        "    #print(bins)\n",
        "    bin_num = -1\n",
        "\n",
        "    if bin_width*0 <= value <= bin_width :\n",
        "      bin_num = 0\n",
        "    elif bin_width*1 < value <= bin_width*2 :\n",
        "      bin_num = 1\n",
        "    elif bin_width*2 < value <= bin_width*3 :\n",
        "      bin_num = 2\n",
        "    elif bin_width*3 < value <= bin_width*4 :\n",
        "      bin_num = 3\n",
        "    elif bin_width*4 < value <= bin_width*5 :\n",
        "      bin_num = 4\n",
        "    #print(value , bin_num)\n",
        "    \n",
        "    return bin_num  \n",
        "  \n",
        "  \n",
        "  def getSentences(self):\n",
        "\n",
        "    with open('/content/datasetSentences.txt','r') as sent:\n",
        "      idc = 0\n",
        "      sc = 0\n",
        "      indexes = []\n",
        "      sentiments = []\n",
        "      lines = []\n",
        "      for lin in sent.readlines():\n",
        "    \n",
        "        end = 0\n",
        "        if idc == 0:\n",
        "          idc += 1\n",
        "          sc += 1\n",
        "          continue\n",
        "        if(re.search(r'\\d+', lin)):\n",
        "          index = re.search(r'\\d+', lin)\n",
        "          end = index.end()\n",
        "          indexes.append(index.group())\n",
        "          idc +=1\n",
        "        if(re.search(r'\\D+', lin)):\n",
        "          sentence = lin[end+1 : ].strip()\n",
        "          #print(sentence)\n",
        "          sentiments.append(sentence)\n",
        "          sc += 1\n",
        "        assert idc == sc      \n",
        "        lines.append(lin)\n",
        "\n",
        "      self.sentencesDF[\"sid\"] = indexes\n",
        "      self.sentencesDF[\"comment\"] = sentiments\n",
        "\n",
        "\n",
        "  def getPhraseIds(self):\n",
        "\n",
        "    with open('/content/dictionary.txt','r') as dif:\n",
        "      for lin in dif.readlines():\n",
        "        tok = re.split(r'\\|',lin)\n",
        "        self.phraseIdDict[tok[0]] = int(tok[1].strip())\n",
        "\n",
        "\n",
        "  def getSentimentLabels(self):\n",
        "\n",
        "    with open('/content/sentiment_labels.txt','r') as lbf:\n",
        "      i = -1\n",
        "      for lin in lbf.readlines():\n",
        "        i += 1\n",
        "        if i == 0:\n",
        "          continue\n",
        "    \n",
        "        tok = re.split(r'\\|',lin)          \n",
        "        self.labelsDict[int((tok[0]))] = float(tok[1].strip())\n",
        "\n",
        "  def getSentenceLabelsDF(self):\n",
        "\n",
        "    sentiments = []\n",
        "    labels = []\n",
        "    i = 0\n",
        "    for cmnt in self.sentencesDF.comment :\n",
        "\n",
        "      if cmnt in self.phraseIdDict:\n",
        "        phraseId = self.phraseIdDict[cmnt]\n",
        "        label = self.labelsDict[phraseId]\n",
        "        sentiments.append(cmnt)\n",
        "        labels.append(self.getLabelIndex(label))\n",
        "        i += 1\n",
        "\n",
        "    assert i > 11000\n",
        "    \n",
        "    self.sentencesLabelDF['sentence'] = sentiments\n",
        "    self.sentencesLabelDF['label'] = labels\n",
        "\n",
        "    return self.sentencesLabelDF\n",
        "\n",
        "  def getSentenceandLabels(self)        :\n",
        "\n",
        "    self.getSentimentLabels()\n",
        "    self.getPhraseIds()\n",
        "    self.getSentences()\n",
        "\n",
        "    return self.getSentenceLabelsDF()\n",
        "\n",
        "\n",
        "  def getSpacyTokens(self , sentence):\n",
        "\n",
        "    nlp = English()\n",
        "    nlp.tokenizer.rules = {key: value for key, value in nlp.tokenizer.rules.items() if \"'\" not in key and \"’\" not in key and \"‘\" not in key}\n",
        "\n",
        "    #  \"nlp\" Object is used to create documents with linguistic annotations.\n",
        "    my_doc = nlp(sentence)\n",
        "\n",
        "    # Create list of word tokens\n",
        "    return [token.text for token in my_doc]\n",
        "\n",
        "\n",
        "  def random_deletion(self , sentence, p=0.3): \n",
        "    global deletecounter\n",
        "    start_time = datetime.now()\n",
        "    words = self.getSpacyTokens(sentence)\n",
        "\n",
        "    if len(words) == 1: # return if single word\n",
        "        return words\n",
        "    remaining = list(filter(lambda x: random.uniform(0,1) > p,words)) \n",
        "    newwords = ''\n",
        "    if len(remaining) == 0: # if not left, sample a random word\n",
        "        newwords = [random.choice(words)] \n",
        "    else:\n",
        "        newwords = remaining\n",
        "\n",
        "    end_time = datetime.now()\n",
        "\n",
        "    time_diff = (end_time - start_time)\n",
        "\n",
        "    execution_time = time_diff.total_seconds() * 1000\n",
        "\n",
        "    deletecounter = deletecounter + 1\n",
        "    print( deletecounter , execution_time)\n",
        "\n",
        "    return ' '.join(newwords)\n",
        "\n",
        "  def random_swap(self , sentence, n=3): \n",
        "    global counter\n",
        "    start_time = datetime.now()\n",
        "\n",
        "    words = self.getSpacyTokens(sentence)\n",
        "\n",
        "    length = range(len(words)) \n",
        "    for _ in range(n):\n",
        "        idx1, idx2 = random.sample(length, 2)\n",
        "        #print(idx1, idx2)\n",
        "        words[idx1], words[idx2] = words[idx2], words[idx1] \n",
        "\n",
        "    end_time = datetime.now()\n",
        "\n",
        "    time_diff = (end_time - start_time)\n",
        "\n",
        "    execution_time = time_diff.total_seconds() * 1000\n",
        "\n",
        "    counter = counter + 1\n",
        "    print( counter , execution_time)\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "  def getReTranslatedSentence(self , sentence):\n",
        "\n",
        "    global translatecounter\n",
        "\n",
        "    start_time = datetime.now()\n",
        "\n",
        "    translator = Translator()\n",
        "\n",
        "    available_langs = list(googletrans.LANGUAGES.keys()) \n",
        "    trans_lang = random.choice(available_langs) \n",
        "    #print(f\"Translating to {googletrans.LANGUAGES[trans_lang]}\")\n",
        "    \n",
        "    #print(sentence)\n",
        "    translations = translator.translate(sentence, dest=trans_lang).text \n",
        "\n",
        "    translations_en_random = translator.translate(translations, src=trans_lang, dest='en').text \n",
        "\n",
        "    #print('back-',translations_en_random)\n",
        "\n",
        "    end_time = datetime.now()\n",
        "\n",
        "    time_diff = (end_time - start_time)\n",
        "\n",
        "    execution_time = time_diff.total_seconds() * 1000\n",
        "\n",
        "    translatecounter = translatecounter + 1\n",
        "    print('tlt ' ,translatecounter , execution_time)\n",
        "\n",
        "    return translations_en_random\n",
        "\n",
        "  def removeStopWord(self, sentence):\n",
        "    nlp = spacy.load('en', parser=False, entity=False)  \n",
        "    nlp.tokenizer.rules = {key: value for key, value in nlp.tokenizer.rules.items() if \"'\" not in key and \"’\" not in key and \"‘\" not in key}\n",
        "    return ' '.join([token.lemma_ for token in nlp(sentence) if not token.is_stop])\n",
        "\n",
        "  def insertSynonymNTimes(self , input , numTimes):\n",
        "\n",
        "    # Load an spacy model (supported models are \"es\" and \"en\") \n",
        "    nlp = spacy.load('en')\n",
        "    nlp.add_pipe(WordnetAnnotator(nlp.lang), after='tagger')\n",
        "    token = nlp('prices')[0]\n",
        "\n",
        "    # wordnet object link spacy token with nltk wordnet interface by giving acces to\n",
        "    # synsets and lemmas \n",
        "    token._.wordnet.synsets()\n",
        "    token._.wordnet.lemmas()\n",
        "\n",
        "    # And automatically tags with wordnet domains\n",
        "    token._.wordnet.wordnet_domains()\n",
        "\n",
        "    # Imagine we want to enrich the following sentence with synonyms\n",
        "    sentence = nlp(input)\n",
        "\n",
        "    # spaCy WordNet lets you find synonyms by domain of interest\n",
        "    # for example economy\n",
        "    #movie_domains = ['banking', 'finance','cinema']\n",
        "    movie_domains = ['film', 'music' , 'celluloid' ,'play', 'sport', 'history' , 'literature', 'banking' , 'finance' ,'cinema']\n",
        "    enriched_sentence = []\n",
        "    \n",
        "\n",
        "    # For each token in the sentence\n",
        "    for token in sentence:\n",
        "      #print('idx-',token.i)\n",
        "      # We get those synsets within the desired domains\n",
        "      synsets = token._.wordnet.wordnet_synsets_for_domain(movie_domains)\n",
        "      #synsets = token._.wordnet.synsets()\n",
        "      if synsets:\n",
        "          lemmas_for_synset = []\n",
        "          for s in synsets:\n",
        "            # If we found a synset in the economy domains\n",
        "            # we get the variants and add them to the enriched sentence\n",
        "            #print(s.lemma_names())\n",
        "            lemmas_for_synset.extend(s.lemma_names())\n",
        "            if (len(lemmas_for_synset) > 1):\n",
        "              #print('lfors-',lemmas_for_synset[1:])\n",
        "              #enriched_sentence.append('({})'.format('|'.join(set(lemmas_for_synset))))\n",
        "              ntimes = (random.choice(lemmas_for_synset[1:]) + ' ')*numTimes\n",
        "              #print(ntimes)\n",
        "              enriched_sentence.append(ntimes)\n",
        "            else:\n",
        "              enriched_sentence.append(token.text)\n",
        "      else:\n",
        "        enriched_sentence.append(token.text)\n",
        "        #mod_sent.append(token.text)\n",
        "\n",
        "    # Let's see our enriched sentence\n",
        "    return ' '.join(enriched_sentence)\n",
        "          \n",
        "\n",
        "  def random_insertion(self , sentence, howmanytimes = 3): \n",
        "\n",
        "    global insertcounter\n",
        "    start_time = datetime.now()\n",
        "\n",
        "    newSentence = self.removeStopWord(sentence) \n",
        "    synmsentence =  self.insertSynonymNTimes(newSentence,howmanytimes)\n",
        "\n",
        "    end_time = datetime.now()\n",
        "\n",
        "    time_diff = (end_time - start_time)\n",
        "\n",
        "    execution_time = time_diff.total_seconds() * 1000\n",
        "    \n",
        "    insertcounter = insertcounter + 1\n",
        "    print('ins', insertcounter , execution_time)\n",
        "\n",
        "    return synmsentence "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZEwRkH6KIOy"
      },
      "source": [
        "### Instantiate the StanfordSentimentAnalysis class and get the comments and label dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhrWwvsUgKyg"
      },
      "source": [
        "sa = StanfordSentimentAnalysis()\n",
        "sldf = sa.getSentenceandLabels()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uVwwRYJJ_RZ"
      },
      "source": [
        "#### save the sentiment analysis dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5A_nPkuFaQp"
      },
      "source": [
        "sldf.to_csv (r'/content/sentanlysdf.csv', index = False, header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NALL9I9gKvA",
        "outputId": "b6180cc6-67be-4e93-a3aa-3219f8ebdc6d"
      },
      "source": [
        "len(sldf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11286"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXiN9cdMJrMy"
      },
      "source": [
        "#### Test random insertion augmentation method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvAdXUSigKrI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "8df80a75-e296-4a89-fcd7-a42cb3ca2ee8"
      },
      "source": [
        "sa.random_insertion (sldf.sentence[11283],3)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ins 1 4507.451\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"' ve view view view  see see see  hippy hippy hippy  - play play play  round round round  round round round  - yuppie plot , ' s enthusiastic charm Fire flame flame flame  do do do  do do do  make make make  make make make  induce induce induce  produce produce produce  get get get  produce produce produce  pull_in pull_in pull_in  bring_in bring_in bring_in  make make make  create create create  take_in take_in take_in  create create create  make make make  realize realize realize  formula fresh .\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5zqR7LFJzTK"
      },
      "source": [
        "#### Test random swap augmentation method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhBbssSo8EcH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "73b99ee2-8fc4-495f-dacb-8440e6ce98d0"
      },
      "source": [
        "sa.random_swap(sldf.sentence[1123])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 326.322\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Writer\\\\/director Joe Carnahan moves s grimy crime drama clunky cliches manual of precinct a , but it ' fast enough to cover its is dialogue and lapses in logic .\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds_UlKLLJ3f3"
      },
      "source": [
        "#### Test random deletion augmentation method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UU02tMhKhBom",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "5446e7f9-425c-4d38-942d-ab4d5d4cb9aa"
      },
      "source": [
        "sa.random_deletion(sldf.sentence[11283])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 336.189\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"' the hippie - turned - plot , there s an enthusiastic charm in Fire that makes the formula again\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuDWGcYkJGHz"
      },
      "source": [
        "### utility function to call different augmentation function\n",
        "\n",
        "#### Input :\n",
        "\n",
        "1.   data frame object\n",
        "2.   augmentation method \n",
        "\n",
        "#### Output:\n",
        "\n",
        "1.   augmented data frame object\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-KCjolOjLUl"
      },
      "source": [
        "def augmentStanfordSentimentAnalysisData(sentAnlysDF , method):\n",
        "\n",
        "  swapSentence = sentAnlysDF.sentence.apply(method)\n",
        "  swapLabel = sentAnlysDF.label\n",
        "\n",
        "  swapDF = pd.concat([swapSentence, swapLabel], axis=1)\n",
        "\n",
        "  return swapDF\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAWMiX0aX9Vs"
      },
      "source": [
        "### Apply the random swap method on each sentence in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLbnsa0TH3Vh"
      },
      "source": [
        "augDF = augmentStanfordSentimentAnalysisData(sldf , sa.random_swap)\n",
        "len(augDF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_CPf4IsUytB"
      },
      "source": [
        "augDF.to_csv (r'/content/augwrapdf.csv', index = False, header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvxJG55WYLpK"
      },
      "source": [
        "### Concatenate the augmented sentences to the original dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK9Lx1XkVI3m"
      },
      "source": [
        "orig_swapDF =  pd.concat([sldf , augDF],ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiY9K4szVSks"
      },
      "source": [
        "orig_swapDF.to_csv (r'/content/orig_wrapdf.csv', index = False, header=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10Z3hBywYJ80"
      },
      "source": [
        "### Apply the random deletion method to each sentence in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lqEhqcOEbHR"
      },
      "source": [
        "augDelDF = augmentStanfordSentimentAnalysisData(sldf , sa.random_deletion)\n",
        "len(augDelDF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTjbM3xbIG6n"
      },
      "source": [
        "augDelDF.to_csv (r'/content/augdeldf.csv', index = False, header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "5nAMd4NpIX4o",
        "outputId": "261c99e2-7d3e-4738-8f3a-ad2e9a034f75"
      },
      "source": [
        "sldf[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Effective but too-tepid biopic</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>If you sometimes like to go to the movies to h...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Emerges as something rare , an issue movie tha...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>The film provides some great insight into the ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Offers that rare combination of entertainment ...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Perhaps no picture ever made has more literall...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Steers turns in a snappy screenplay that curls...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>But he somehow pulls it off .</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  label\n",
              "0  The Rock is destined to be the 21st Century 's...      3\n",
              "1  The gorgeously elaborate continuation of `` Th...      4\n",
              "2                     Effective but too-tepid biopic      2\n",
              "3  If you sometimes like to go to the movies to h...      3\n",
              "4  Emerges as something rare , an issue movie tha...      4\n",
              "5  The film provides some great insight into the ...      2\n",
              "6  Offers that rare combination of entertainment ...      4\n",
              "7  Perhaps no picture ever made has more literall...      3\n",
              "8  Steers turns in a snappy screenplay that curls...      3\n",
              "9                      But he somehow pulls it off .      3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rggFaRBREbD9",
        "outputId": "d0be4369-725a-47a0-b590-7d1c03edfe37"
      },
      "source": [
        "len(orig_swapDF)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22572"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnmXst1dYkTB"
      },
      "source": [
        "### Concatenate the modified sentence to the dataset which has original + words swapped in the sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ghqz7ifEbAx"
      },
      "source": [
        "orig_swap_delDF =  pd.concat([orig_swapDF , augDelDF],ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28XDUVbbYf6r",
        "outputId": "15ed6cf1-364d-498c-e601-6eeb51409dea"
      },
      "source": [
        "len(orig_swap_delDF)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33858"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sqCcPaoXUd4"
      },
      "source": [
        "orig_swap_delDF.to_csv (r'/content/orig_wrap_deldf.csv', index = False, header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uREnVyu2Y4Cq"
      },
      "source": [
        "### Apply the google retranslation method to each sentence in the original dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zug8fd6ju7FZ"
      },
      "source": [
        "augtrnsltDF = augmentStanfordSentimentAnalysisData(sldf , sa.getReTranslatedSentence)\n",
        "len(augtrnsltDF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOHdOEtgu69S"
      },
      "source": [
        "augtrnsltDF.to_csv (r'/content/aug_tltdf.csv', index = False, header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "actR8zfCZEIy"
      },
      "source": [
        "### Concatenate the translated sentences to original + swapped + deleted dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xkrZdzYu6zH"
      },
      "source": [
        "orig_swap_del_tltDF =  pd.concat([orig_swap_delDF , augtrnsltDF],ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5kAUWEqu6qy"
      },
      "source": [
        "orig_swap_del_tltDF.to_csv (r'/content/orig_wrap_del_tltdf.csv', index = False, header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szPC1sO4ZQzi"
      },
      "source": [
        "### Apply the random insertion method to each sentence in the original dataset.This was the most expensive operation.It took an avg of 2500 milliseconds to do random insertion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suNgeumJJuZI",
        "outputId": "692e2d1b-999b-42b5-f89f-fc6b8383f767"
      },
      "source": [
        "auginsertDF = augmentStanfordSentimentAnalysisData(sldf , sa.random_insertion)\n",
        "len(auginsertDF)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ins 1 2039.52\n",
            "ins 2 2132.431\n",
            "ins 3 2142.6620000000003\n",
            "ins 4 1884.3229999999999\n",
            "ins 5 2089.708\n",
            "ins 6 1923.519\n",
            "ins 7 2052.1240000000003\n",
            "ins 8 1879.496\n",
            "ins 9 2130.5930000000003\n",
            "ins 10 1908.346\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11286"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "KGjkENwcJuWh",
        "outputId": "a3204cbe-69f9-49a2-bca0-914c34d6d3dd"
      },
      "source": [
        "auginsertDF.to_csv (r'/content/aug_insdf.csv', index = False, header=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The rock is in the destiny of the 21st century...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The elaborate continuation of `` The Lord of t...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Effective but tepid biopic</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Sometimes if you want to travel to the movies,...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Something comes out and it observes a fair and...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11281</th>\n",
              "      <td>Repeat real.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11282</th>\n",
              "      <td>Not surprised.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11283</th>\n",
              "      <td>We have seen the hippy-turned-jopy plot before...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11284</th>\n",
              "      <td>Her lovers face poetry with poetry like '' and...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11285</th>\n",
              "      <td>In this case zero.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11286 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                sentence  label\n",
              "0      The rock is in the destiny of the 21st century...      3\n",
              "1      The elaborate continuation of `` The Lord of t...      4\n",
              "2                             Effective but tepid biopic      2\n",
              "3      Sometimes if you want to travel to the movies,...      3\n",
              "4      Something comes out and it observes a fair and...      4\n",
              "...                                                  ...    ...\n",
              "11281                                       Repeat real.      0\n",
              "11282                                     Not surprised.      1\n",
              "11283  We have seen the hippy-turned-jopy plot before...      3\n",
              "11284  Her lovers face poetry with poetry like '' and...      0\n",
              "11285                                 In this case zero.      1\n",
              "\n",
              "[11286 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbhA_jHJZn92"
      },
      "source": [
        "Concatenate the randomly inserted words in the sentence to original + swapped + deleted + translated dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eT_ZpIViJuTU"
      },
      "source": [
        "orig_swap_del_tlt_insDF =  pd.concat([orig_swap_del_tltDF , auginsertDF],ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCCZ2BM1JuQE"
      },
      "source": [
        "orig_swap_del_tlt_insDF.to_csv (r'/content/orig_swap_del_tlt_insDF.csv', index = False, header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gqOo7frAEU5",
        "outputId": "1928f048-3de8-4e0a-fe22-9e3fb36ff9d2"
      },
      "source": [
        "!python -m spacy info"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m\n",
            "============================== Info about spaCy ==============================\u001b[0m\n",
            "\n",
            "spaCy version    2.2.4                         \n",
            "Location         /usr/local/lib/python3.7/dist-packages/spacy\n",
            "Platform         Linux-5.4.109+-x86_64-with-Ubuntu-18.04-bionic\n",
            "Python version   3.7.10                        \n",
            "Models           en                            \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}