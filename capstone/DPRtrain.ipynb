{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DPRtrain.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMEJmJTW84d1"
      },
      "source": [
        "https://github.com/luyug/GC-DPR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du3LYIFgUcxQ"
      },
      "source": [
        "This code is sourced from https://github.com/luyug/GC-DPR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbZvvKkq65ky",
        "outputId": "a2faf350-1204-4bdb-af6a-61768477b615"
      },
      "source": [
        "!git clone https://github.com/luyug/GC-DPR.git\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GC-DPR'...\n",
            "remote: Enumerating objects: 73, done.\u001b[K\n",
            "remote: Counting objects: 100% (73/73), done.\u001b[K\n",
            "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
            "remote: Total 73 (delta 33), reused 58 (delta 24), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (73/73), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIv-bOAm7OUZ",
        "outputId": "9b97d462-aedc-4990-b5c2-28c58217ac05"
      },
      "source": [
        "%cd GC-DPR\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GC-DPR\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nuuW0snVKe5"
      },
      "source": [
        "### Install the required dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJ8iTlTZ7SXI",
        "outputId": "fa793b4b-4ffc-4a30-cf3a-1518dba1cade"
      },
      "source": [
        "!pip install ."
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/GC-DPR\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gc-dpr==0.1.0) (0.29.24)\n",
            "Collecting faiss-cpu>=1.6.1\n",
            "  Downloading faiss_cpu-1.7.1.post2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.4 MB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gc-dpr==0.1.0) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from gc-dpr==0.1.0) (1.19.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from gc-dpr==0.1.0) (2019.12.20)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from gc-dpr==0.1.0) (1.9.0+cu102)\n",
            "Collecting transformers<3.1.0,>=3.0.0\n",
            "  Downloading transformers-3.0.2-py3-none-any.whl (769 kB)\n",
            "\u001b[K     |████████████████████████████████| 769 kB 59.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from gc-dpr==0.1.0) (4.62.0)\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Requirement already satisfied: spacy>=2.1.8 in /usr/local/lib/python3.7/dist-packages (from gc-dpr==0.1.0) (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.8->gc-dpr==0.1.0) (3.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.8->gc-dpr==0.1.0) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.8->gc-dpr==0.1.0) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.8->gc-dpr==0.1.0) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.8->gc-dpr==0.1.0) (57.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.8->gc-dpr==0.1.0) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.8->gc-dpr==0.1.0) (2.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.8->gc-dpr==0.1.0) (1.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.8->gc-dpr==0.1.0) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.8->gc-dpr==0.1.0) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.8->gc-dpr==0.1.0) (0.8.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.1.8->gc-dpr==0.1.0) (4.6.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.1.8->gc-dpr==0.1.0) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.1.8->gc-dpr==0.1.0) (3.7.4.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.8->gc-dpr==0.1.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.8->gc-dpr==0.1.0) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.8->gc-dpr==0.1.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.8->gc-dpr==0.1.0) (3.0.4)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 81.2 MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc1\n",
            "  Downloading tokenizers-0.8.1rc1-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 55.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<3.1.0,>=3.0.0->gc-dpr==0.1.0) (21.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 53.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<3.1.0,>=3.0.0->gc-dpr==0.1.0) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<3.1.0,>=3.0.0->gc-dpr==0.1.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<3.1.0,>=3.0.0->gc-dpr==0.1.0) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<3.1.0,>=3.0.0->gc-dpr==0.1.0) (1.15.0)\n",
            "Building wheels for collected packages: gc-dpr, wget\n",
            "  Building wheel for gc-dpr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gc-dpr: filename=gc_dpr-0.1.0-py3-none-any.whl size=13473 sha256=502b8d57187028b4ded070ae45132c5891e06844a85660f01d32668116610911\n",
            "  Stored in directory: /root/.cache/pip/wheels/44/ed/1a/5ed8a28e58c8ffe7d418562663c5873cf2b5241cca84bae786\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9672 sha256=1286e40a81a4ff717d5b4ce6ae81d5a7ec98c5d74f9d940050cfabc02e46de66\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built gc-dpr wget\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, wget, transformers, faiss-cpu, gc-dpr\n",
            "Successfully installed faiss-cpu-1.7.1.post2 gc-dpr-0.1.0 sacremoses-0.0.45 sentencepiece-0.1.96 tokenizers-0.8.1rc1 transformers-3.0.2 wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTApmUZQVPlS"
      },
      "source": [
        "### Load sample datasset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOS5afSg8vX4",
        "outputId": "178623af-ebf0-431b-ea35-956f55013e46"
      },
      "source": [
        "#'data.retriever.nq-train'\n",
        "!python data/download_data.py \\\n",
        "   --resource {'data.retriever.nq-dev'}  \\\n",
        "   --output_dir {'/content/dprdevdata'}] "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading from  https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-nq-dev.json.gz\n",
            "Saved to  /content/dprdevdata]/data/retriever/nq-dev.tmp\n",
            "Uncompressing  /content/dprdevdata]/data/retriever/nq-dev.tmp\n",
            "Saved to  /content/dprdevdata]/data/retriever/nq-dev.json\n",
            "Loading from  https://dl.fbaipublicfiles.com/dpr/nq_license/LICENSE\n",
            "Saved to  /content/dprdevdata]/data/retriever/LICENSE\n",
            "Loading from  https://dl.fbaipublicfiles.com/dpr/nq_license/README\n",
            "Saved to  /content/dprdevdata]/data/retriever/README\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HxyHJZxVVRk"
      },
      "source": [
        "### Train the model using the sample dataset \n",
        "\n",
        "* This code will be used to build the end to end dpr model for the test project\n",
        "\n",
        "* here a test run is done with the sample data to check for any issue , the code will be refactored to use with the capstone dataset , that we have generated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqipO-jU8vT7",
        "outputId": "eacfb1ff-bfb5-4e24-ab33-fabd24e4f4a9"
      },
      "source": [
        "!python train_dense_encoder.py  \\\n",
        "   --encoder_model_type {'hf_bert' }  \\\n",
        "   --pretrained_model_cfg {'bert-base-uncased'}  \\\n",
        "   --train_file {'/content/dprdevdata]/data/retriever/*.json'}   \\\n",
        "   --dev_file {'/content/dprdevdata]/data/retriever/*.json'}  \\\n",
        "   --output_dir {'/content/dprdevdata]/chkpnt'}  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized host ae8ce1557aeb as d.rank -1 on device=cuda, n_gpu=1, world size=1\n",
            "16-bits training: False \n",
            " **************** CONFIGURATION **************** \n",
            "adam_betas                     -->   (0.9, 0.999)\n",
            "adam_eps                       -->   1e-08\n",
            "batch_size                     -->   2\n",
            "checkpoint_file_name           -->   dpr_biencoder\n",
            "ctx_chunk_size                 -->   8\n",
            "dev_batch_size                 -->   4\n",
            "dev_file                       -->   /content/dprdevdata]/data/retriever/nq-dev.json\n",
            "device                         -->   cuda\n",
            "distributed_world_size         -->   1\n",
            "do_lower_case                  -->   False\n",
            "dropout                        -->   0.1\n",
            "encoder_model_type             -->   hf_bert\n",
            "eval_per_epoch                 -->   1\n",
            "fix_ctx_encoder                -->   False\n",
            "fp16                           -->   False\n",
            "fp16_opt_level                 -->   O1\n",
            "global_loss_buf_sz             -->   150000\n",
            "grad_cache                     -->   False\n",
            "gradient_accumulation_steps    -->   1\n",
            "hard_negatives                 -->   1\n",
            "learning_rate                  -->   1e-05\n",
            "local_rank                     -->   -1\n",
            "log_batch_step                 -->   100\n",
            "max_grad_norm                  -->   1.0\n",
            "model_file                     -->   None\n",
            "n_gpu                          -->   1\n",
            "no_cuda                        -->   False\n",
            "num_train_epochs               -->   3.0\n",
            "other_negatives                -->   0\n",
            "output_dir                     -->   /content/dprdevdata]/chkpnt\n",
            "pretrained_file                -->   None\n",
            "pretrained_model_cfg           -->   bert-base-uncased\n",
            "projection_dim                 -->   0\n",
            "q_chunk_size                   -->   16\n",
            "seed                           -->   0\n",
            "sequence_length                -->   512\n",
            "shuffle_positive_ctx           -->   False\n",
            "train_file                     -->   /content/dprdevdata]/data/retriever/nq-dev.json\n",
            "train_files_upsample_rates     -->   None\n",
            "train_rolling_loss_step        -->   100\n",
            "val_av_rank_bsz                -->   128\n",
            "val_av_rank_hard_neg           -->   30\n",
            "val_av_rank_max_qs             -->   10000\n",
            "val_av_rank_other_neg          -->   30\n",
            "val_av_rank_start_epoch        -->   10000\n",
            "warmup_steps                   -->   100\n",
            "weight_decay                   -->   0.0\n",
            " **************** CONFIGURATION **************** \n",
            "***** Initializing components for training *****\n",
            "Checkpoint files []\n",
            "PyTorch version 1.9.0+cu102 available.\n",
            "TensorFlow version 2.6.0 available.\n",
            "Lock 140211685560656 acquired on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
            "https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpbrjnlo7e\n",
            "Downloading: 100% 433/433 [00:00<00:00, 427kB/s]\n",
            "storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json in cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "creating metadata file for /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "Lock 140211685560656 released on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
            "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "Lock 140209789913552 acquired on /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
            "https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpy1lold03\n",
            "Downloading: 100% 440M/440M [00:07<00:00, 60.2MB/s]\n",
            "storing https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin in cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "creating metadata file for /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "Lock 140209789913552 released on /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
            "loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "All model checkpoint weights were used when initializing HFBertEncoder.\n",
            "\n",
            "All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.\n",
            "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "All model checkpoint weights were used when initializing HFBertEncoder.\n",
            "\n",
            "All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.\n",
            "Lock 140209764791248 acquired on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
            "https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpumx_yoz1\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 954kB/s]\n",
            "storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt in cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "creating metadata file for /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "Lock 140209764791248 released on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
            "loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "Reading file /content/dprdevdata]/data/retriever/nq-dev.json\n",
            "Aggregated data size: 6515\n",
            "Total cleaned data size: 6515\n",
            "  Total iterations per epoch=3258\n",
            " Total updates=9774\n",
            "  Eval step = 3258\n",
            "***** Training *****\n",
            "***** Epoch 0 *****\n",
            "Epoch: 0: Step: 1/3258, loss=20.879175, lr=0.000000\n",
            "Train batch 100\n",
            "Avg. loss per last 100 batches: 7.063016\n",
            "Epoch: 0: Step: 101/3258, loss=3.845323, lr=0.000010\n",
            "Train batch 200\n",
            "Avg. loss per last 100 batches: 1.425502\n",
            "Epoch: 0: Step: 201/3258, loss=3.341021, lr=0.000010\n",
            "Train batch 300\n",
            "Avg. loss per last 100 batches: 0.856511\n",
            "Epoch: 0: Step: 301/3258, loss=0.453331, lr=0.000010\n",
            "Train batch 400\n",
            "Avg. loss per last 100 batches: 0.840076\n",
            "Epoch: 0: Step: 401/3258, loss=0.182937, lr=0.000010\n",
            "Train batch 500\n",
            "Avg. loss per last 100 batches: 0.836007\n",
            "Epoch: 0: Step: 501/3258, loss=0.451287, lr=0.000010\n",
            "Train batch 600\n",
            "Avg. loss per last 100 batches: 0.727061\n",
            "Epoch: 0: Step: 601/3258, loss=0.101555, lr=0.000009\n",
            "Train batch 700\n",
            "Avg. loss per last 100 batches: 0.715019\n",
            "Epoch: 0: Step: 701/3258, loss=0.779977, lr=0.000009\n",
            "Train batch 800\n",
            "Avg. loss per last 100 batches: 0.665379\n",
            "Epoch: 0: Step: 801/3258, loss=0.000054, lr=0.000009\n",
            "Train batch 900\n",
            "Avg. loss per last 100 batches: 0.554073\n",
            "Epoch: 0: Step: 901/3258, loss=0.652408, lr=0.000009\n",
            "Train batch 1000\n",
            "Avg. loss per last 100 batches: 0.699420\n",
            "Epoch: 0: Step: 1001/3258, loss=0.006642, lr=0.000009\n",
            "Train batch 1100\n",
            "Avg. loss per last 100 batches: 0.736018\n",
            "Epoch: 0: Step: 1101/3258, loss=3.435383, lr=0.000009\n",
            "Train batch 1200\n",
            "Avg. loss per last 100 batches: 0.612892\n",
            "Epoch: 0: Step: 1201/3258, loss=0.002450, lr=0.000009\n",
            "Train batch 1300\n",
            "Avg. loss per last 100 batches: 0.694482\n",
            "Epoch: 0: Step: 1301/3258, loss=0.539028, lr=0.000009\n",
            "Train batch 1400\n",
            "Avg. loss per last 100 batches: 0.786224\n",
            "Epoch: 0: Step: 1401/3258, loss=0.022790, lr=0.000009\n",
            "Train batch 1500\n",
            "Avg. loss per last 100 batches: 0.482405\n",
            "Epoch: 0: Step: 1501/3258, loss=0.040423, lr=0.000009\n",
            "Train batch 1600\n",
            "Avg. loss per last 100 batches: 0.487796\n",
            "Epoch: 0: Step: 1601/3258, loss=0.008157, lr=0.000008\n",
            "Train batch 1700\n",
            "Avg. loss per last 100 batches: 0.753921\n",
            "Epoch: 0: Step: 1701/3258, loss=0.001043, lr=0.000008\n",
            "Train batch 1800\n",
            "Avg. loss per last 100 batches: 0.702689\n",
            "Epoch: 0: Step: 1801/3258, loss=0.323058, lr=0.000008\n",
            "Train batch 1900\n",
            "Avg. loss per last 100 batches: 0.670858\n",
            "Epoch: 0: Step: 1901/3258, loss=0.534937, lr=0.000008\n",
            "Epoch: 0: Step: 2001/3258, loss=1.180499, lr=0.000008\n",
            "Train batch 2100\n",
            "Avg. loss per last 100 batches: 0.558530\n",
            "Epoch: 0: Step: 2101/3258, loss=0.003663, lr=0.000008\n",
            "Train batch 2200\n",
            "Avg. loss per last 100 batches: 0.578332\n",
            "Epoch: 0: Step: 2201/3258, loss=0.718293, lr=0.000008\n",
            "Train batch 2300\n",
            "Avg. loss per last 100 batches: 0.623423\n",
            "Epoch: 0: Step: 2301/3258, loss=0.096823, lr=0.000008\n",
            "Train batch 2400\n",
            "Avg. loss per last 100 batches: 0.729157\n",
            "Epoch: 0: Step: 2401/3258, loss=0.107544, lr=0.000008\n",
            "Train batch 2500\n",
            "Avg. loss per last 100 batches: 0.408302\n",
            "Epoch: 0: Step: 2501/3258, loss=0.095298, lr=0.000008\n",
            "Train batch 2600\n",
            "Avg. loss per last 100 batches: 0.536457\n",
            "Epoch: 0: Step: 2601/3258, loss=0.033315, lr=0.000007\n",
            "Train batch 2700\n",
            "Avg. loss per last 100 batches: 0.444174\n",
            "Epoch: 0: Step: 2701/3258, loss=0.119821, lr=0.000007\n",
            "Train batch 2800\n",
            "Avg. loss per last 100 batches: 0.374485\n",
            "Epoch: 0: Step: 2801/3258, loss=7.624740, lr=0.000007\n",
            "Train batch 2900\n",
            "Avg. loss per last 100 batches: 0.842974\n",
            "Epoch: 0: Step: 2901/3258, loss=0.000828, lr=0.000007\n",
            "Train batch 3000\n",
            "Avg. loss per last 100 batches: 0.526776\n",
            "Epoch: 0: Step: 3001/3258, loss=0.022878, lr=0.000007\n",
            "Train batch 3100\n",
            "Avg. loss per last 100 batches: 0.435324\n",
            "Epoch: 0: Step: 3101/3258, loss=0.003947, lr=0.000007\n",
            "Train batch 3200\n",
            "Avg. loss per last 100 batches: 0.605610\n",
            "Epoch: 0: Step: 3201/3258, loss=0.000000, lr=0.000007\n",
            "Finished iterating, iteration=3258, shard=0\n",
            "Validation: Epoch: 0 Step: 3258/3258\n",
            "NLL validation ...\n",
            "Reading file /content/dprdevdata]/data/retriever/nq-dev.json\n",
            "Aggregated data size: 6515\n",
            "Total cleaned data size: 6515\n",
            "Eval step: 99 , used_time=46.501453 sec., loss=0.112810 \n",
            "Eval step: 199 , used_time=92.548126 sec., loss=0.000004 \n",
            "Eval step: 299 , used_time=138.959932 sec., loss=1.898458 \n",
            "Eval step: 399 , used_time=185.267789 sec., loss=0.073006 \n",
            "Eval step: 499 , used_time=231.151421 sec., loss=0.002159 \n",
            "Eval step: 599 , used_time=277.626225 sec., loss=2.268908 \n",
            "Eval step: 699 , used_time=323.639210 sec., loss=0.857368 \n",
            "Eval step: 799 , used_time=370.085279 sec., loss=5.117107 \n",
            "Eval step: 899 , used_time=416.501861 sec., loss=0.295370 \n",
            "Eval step: 999 , used_time=462.787835 sec., loss=3.934539 \n",
            "Eval step: 1099 , used_time=508.942142 sec., loss=0.146630 \n",
            "Eval step: 1199 , used_time=555.113686 sec., loss=3.845603 \n",
            "Eval step: 1299 , used_time=601.293154 sec., loss=0.729770 \n",
            "Eval step: 1399 , used_time=647.605483 sec., loss=2.340352 \n",
            "Eval step: 1499 , used_time=694.008654 sec., loss=0.328067 \n",
            "Eval step: 1599 , used_time=740.507991 sec., loss=0.707699 \n",
            "NLL Validation: loss = 0.735069. correct prediction ratio  5307/6516 ~  0.814457\n",
            "Saved checkpoint at /content/dprdevdata]/chkpnt/dpr_biencoder.0.3258\n",
            "Saved checkpoint to /content/dprdevdata]/chkpnt/dpr_biencoder.0.3258\n",
            "New Best validation checkpoint /content/dprdevdata]/chkpnt/dpr_biencoder.0.3258\n",
            "NLL validation ...\n",
            "Reading file /content/dprdevdata]/data/retriever/nq-dev.json\n",
            "Aggregated data size: 6515\n",
            "Total cleaned data size: 6515\n",
            "Eval step: 99 , used_time=47.291642 sec., loss=0.112810 \n",
            "Eval step: 199 , used_time=92.924058 sec., loss=0.000004 \n",
            "Eval step: 299 , used_time=139.353049 sec., loss=1.898458 \n",
            "Eval step: 399 , used_time=185.509704 sec., loss=0.073006 \n",
            "Eval step: 499 , used_time=231.859500 sec., loss=0.002159 \n",
            "Eval step: 599 , used_time=278.384533 sec., loss=2.268908 \n",
            "Eval step: 699 , used_time=324.808600 sec., loss=0.857368 \n",
            "Eval step: 799 , used_time=371.256480 sec., loss=5.117107 \n",
            "Eval step: 899 , used_time=417.649062 sec., loss=0.295370 \n",
            "Eval step: 999 , used_time=463.921428 sec., loss=3.934539 \n",
            "Eval step: 1099 , used_time=510.205219 sec., loss=0.146630 \n",
            "Eval step: 1199 , used_time=556.502730 sec., loss=3.845603 \n",
            "Eval step: 1299 , used_time=602.778700 sec., loss=0.729770 \n",
            "Eval step: 1399 , used_time=649.054720 sec., loss=2.340352 \n",
            "Eval step: 1499 , used_time=695.357217 sec., loss=0.328067 \n",
            "Eval step: 1599 , used_time=741.659300 sec., loss=0.707699 \n",
            "NLL Validation: loss = 0.735069. correct prediction ratio  5307/6516 ~  0.814457\n",
            "Saved checkpoint at /content/dprdevdata]/chkpnt/dpr_biencoder.0.3258\n",
            "Saved checkpoint to /content/dprdevdata]/chkpnt/dpr_biencoder.0.3258\n",
            "Av Loss per epoch=0.848116\n",
            "epoch total correct predictions=5272\n",
            "***** Epoch 1 *****\n",
            "Epoch: 1: Step: 1/3258, loss=0.040184, lr=0.000007\n",
            "Train batch 100\n",
            "Avg. loss per last 100 batches: 0.288920\n",
            "Epoch: 1: Step: 101/3258, loss=1.093456, lr=0.000007\n",
            "Train batch 200\n",
            "Avg. loss per last 100 batches: 0.489996\n",
            "Epoch: 1: Step: 201/3258, loss=0.095779, lr=0.000007\n",
            "Train batch 300\n",
            "Avg. loss per last 100 batches: 0.454743\n",
            "Epoch: 1: Step: 301/3258, loss=0.000084, lr=0.000006\n",
            "Train batch 400\n",
            "Avg. loss per last 100 batches: 0.254579\n",
            "Epoch: 1: Step: 401/3258, loss=0.000007, lr=0.000006\n",
            "Train batch 500\n",
            "Avg. loss per last 100 batches: 0.455086\n",
            "Epoch: 1: Step: 501/3258, loss=0.000008, lr=0.000006\n",
            "Train batch 600\n",
            "Avg. loss per last 100 batches: 0.409794\n",
            "Epoch: 1: Step: 601/3258, loss=0.001803, lr=0.000006\n",
            "Train batch 700\n",
            "Avg. loss per last 100 batches: 0.430022\n",
            "Epoch: 1: Step: 701/3258, loss=0.000130, lr=0.000006\n",
            "Train batch 800\n",
            "Avg. loss per last 100 batches: 0.444044\n",
            "Epoch: 1: Step: 801/3258, loss=6.862292, lr=0.000006\n",
            "Train batch 900\n",
            "Avg. loss per last 100 batches: 0.576271\n",
            "Epoch: 1: Step: 901/3258, loss=0.000055, lr=0.000006\n",
            "Train batch 1000\n",
            "Avg. loss per last 100 batches: 0.334279\n",
            "Epoch: 1: Step: 1001/3258, loss=0.009849, lr=0.000006\n",
            "Train batch 1100\n",
            "Avg. loss per last 100 batches: 0.227425\n",
            "Epoch: 1: Step: 1101/3258, loss=0.000199, lr=0.000006\n",
            "Train batch 1200\n",
            "Avg. loss per last 100 batches: 0.447041\n",
            "Epoch: 1: Step: 1201/3258, loss=0.698287, lr=0.000005\n",
            "Train batch 1300\n",
            "Avg. loss per last 100 batches: 0.367504\n",
            "Epoch: 1: Step: 1301/3258, loss=0.002227, lr=0.000005\n",
            "Train batch 1400\n",
            "Avg. loss per last 100 batches: 0.253053\n",
            "Epoch: 1: Step: 1401/3258, loss=0.000101, lr=0.000005\n",
            "Train batch 1500\n",
            "Avg. loss per last 100 batches: 0.234293\n",
            "Epoch: 1: Step: 1501/3258, loss=0.617722, lr=0.000005\n",
            "Train batch 1600\n",
            "Avg. loss per last 100 batches: 0.416256\n",
            "Epoch: 1: Step: 1601/3258, loss=0.000360, lr=0.000005\n",
            "Train batch 1700\n",
            "Avg. loss per last 100 batches: 0.321974\n",
            "Epoch: 1: Step: 1701/3258, loss=0.000140, lr=0.000005\n",
            "Train batch 1800\n",
            "Avg. loss per last 100 batches: 0.329026\n",
            "Epoch: 1: Step: 1801/3258, loss=0.000017, lr=0.000005\n",
            "Train batch 1900\n",
            "Avg. loss per last 100 batches: 0.302438\n",
            "Epoch: 1: Step: 1901/3258, loss=0.000803, lr=0.000005\n",
            "Train batch 2000\n",
            "Avg. loss per last 100 batches: 0.375625\n",
            "Epoch: 1: Step: 2001/3258, loss=0.000011, lr=0.000005\n",
            "Train batch 2100\n",
            "Avg. loss per last 100 batches: 0.358913\n",
            "Epoch: 1: Step: 2101/3258, loss=0.000000, lr=0.000005\n",
            "Train batch 2200\n",
            "Avg. loss per last 100 batches: 0.222720\n",
            "Epoch: 1: Step: 2201/3258, loss=0.000362, lr=0.000004\n",
            "Train batch 2300\n",
            "Avg. loss per last 100 batches: 0.356647\n",
            "Epoch: 1: Step: 2301/3258, loss=0.000000, lr=0.000004\n",
            "Train batch 2400\n",
            "Avg. loss per last 100 batches: 0.383229\n",
            "Epoch: 1: Step: 2401/3258, loss=0.000018, lr=0.000004\n",
            "Train batch 2500\n",
            "Avg. loss per last 100 batches: 0.293126\n",
            "Epoch: 1: Step: 2501/3258, loss=0.000080, lr=0.000004\n",
            "Train batch 2600\n",
            "Avg. loss per last 100 batches: 0.386326\n",
            "Epoch: 1: Step: 2601/3258, loss=0.000000, lr=0.000004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PW8kfFEb8vQp"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    }
  ]
}